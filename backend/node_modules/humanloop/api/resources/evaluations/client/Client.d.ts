/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as environments from "../../../../environments";
import * as core from "../../../../core";
import * as Humanloop from "../../../index";
export declare namespace Evaluations {
    interface Options {
        environment?: core.Supplier<environments.HumanloopEnvironment | string>;
        /** Specify a custom URL to connect the client to. */
        baseUrl?: core.Supplier<string>;
        apiKey?: core.Supplier<string>;
        fetcher?: core.FetchFunction;
    }
    interface RequestOptions {
        /** The maximum time to wait for a response in seconds. */
        timeoutInSeconds?: number;
        /** The number of times to retry the request. Defaults to 2. */
        maxRetries?: number;
        /** A hook to abort the request. */
        abortSignal?: AbortSignal;
        /** Additional headers to include in the request. */
        headers?: Record<string, string>;
    }
}
/**
 * Evaluations help you measure the performance of your Prompts.
 *
 * An Evaluation consists of Runs and a set of Evaluators. These Evaluators judge the Logs in all Runs,
 * allowing you to compare the performance of different Prompts.
 * A Run can be created with a specific Dataset and Version, or by specifying the Logs directly.
 * If a Run is created with a Dataset and Version, Humanloop will automatically start generating Logs, and
 * the Evaluators will then be run on these Logs.
 *
 * Aggregate stats can be viewed in the Humanloop app or retrieved with the **Get Evaluation Stats** endpoint.
 *
 * Note that when a Run is created, Humanloop will attempt to reuse any existing Logs for each Datapoint-Version
 * pair. This means that you can create Runs without generating new Logs unnecessarily.
 */
export declare class Evaluations {
    protected readonly _options: Evaluations.Options;
    constructor(_options?: Evaluations.Options);
    /**
     * Retrieve a list of Evaluations for the specified File.
     *
     * @param {Humanloop.ListEvaluationsGetRequest} request
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.list({
     *         fileId: "pr_30gco7dx6JDq4200GVOHa",
     *         size: 1
     *     })
     */
    list(request: Humanloop.ListEvaluationsGetRequest, requestOptions?: Evaluations.RequestOptions): Promise<core.Page<Humanloop.EvaluationResponse>>;
    /**
     * Create an Evaluation.
     *
     * Create a new Evaluation by specifying the File to evaluate, and a name
     * for the Evaluation.
     * You can then add Runs to this Evaluation using the `POST /evaluations/{id}/runs` endpoint.
     *
     * @param {Humanloop.CreateEvaluationRequest} request
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.create({
     *         evaluators: [{
     *                 versionId: "version_id"
     *             }]
     *     })
     */
    create(request: Humanloop.CreateEvaluationRequest, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationResponse>;
    /**
     * Add Evaluators to an Evaluation.
     *
     * The Evaluators will be run on the Logs generated for the Evaluation.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {Humanloop.AddEvaluatorsRequest} request
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.addEvaluators("id", {
     *         evaluators: [{
     *                 versionId: "version_id"
     *             }]
     *     })
     */
    addEvaluators(id: string, request: Humanloop.AddEvaluatorsRequest, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationResponse>;
    /**
     * Remove an Evaluator from an Evaluation.
     *
     * The Evaluator will no longer be run on the Logs in the Evaluation.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {string} evaluatorVersionId - Unique identifier for Evaluator Version.
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.removeEvaluator("id", "evaluator_version_id")
     */
    removeEvaluator(id: string, evaluatorVersionId: string, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationResponse>;
    /**
     * Get an Evaluation.
     *
     * This includes the Evaluators associated with the Evaluation and metadata about the Evaluation,
     * such as its name.
     *
     * To get the Runs associated with the Evaluation, use the `GET /evaluations/{id}/runs` endpoint.
     * To retrieve stats for the Evaluation, use the `GET /evaluations/{id}/stats` endpoint.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.get("ev_567yza")
     */
    get(id: string, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationResponse>;
    /**
     * Delete an Evaluation.
     *
     * The Runs and Evaluators in the Evaluation will not be deleted.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.delete("ev_567yza")
     */
    delete(id: string, requestOptions?: Evaluations.RequestOptions): Promise<void>;
    /**
     * List all Runs for an Evaluation.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.listRunsForEvaluation("id")
     */
    listRunsForEvaluation(id: string, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationRunsResponse>;
    /**
     * Create an Evaluation Run.
     *
     * Optionally specify the Dataset and version to be evaluated.
     *
     * Humanloop will automatically start generating Logs and running Evaluators where
     * `orchestrated=true`. If you are generating Logs yourself, you can set `orchestrated=false`
     * and then generate and submit the required Logs via the API.
     *
     * If `dataset` and `version` are provided, you can set `use_existing_logs=True` to reuse existing Logs,
     * avoiding generating new Logs unnecessarily. Logs that are associated with the specified Version and have `source_datapoint_id`
     * referencing a datapoint in the specified Dataset will be associated with the Run.
     *
     * To keep updated on the progress of the Run, you can poll the Run using
     * the `GET /evaluations/{id}/runs` endpoint and check its status.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {Humanloop.CreateRunRequest} request
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.createRun("id")
     */
    createRun(id: string, request?: Humanloop.CreateRunRequest, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationRunResponse>;
    /**
     * Add an existing Run to the specified Evaluation.
     *
     * This is useful if you want to compare the Runs in this Evaluation with an existing Run
     * that exists within another Evaluation.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {string} runId - Unique identifier for Run.
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.addExistingRun("id", "run_id")
     */
    addExistingRun(id: string, runId: string, requestOptions?: Evaluations.RequestOptions): Promise<unknown>;
    /**
     * Remove a Run from an Evaluation.
     *
     * The Logs and Versions used in the Run will not be deleted.
     * If this Run is used in any other Evaluations, it will still be available in those Evaluations.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {string} runId - Unique identifier for Run.
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.removeRun("id", "run_id")
     */
    removeRun(id: string, runId: string, requestOptions?: Evaluations.RequestOptions): Promise<void>;
    /**
     * Update an Evaluation Run.
     *
     * Specify `control=true` to use this Run as the control Run for the Evaluation.
     * You can cancel a running/pending Run, or mark a Run that uses external or human Evaluators as completed.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {string} runId - Unique identifier for Run.
     * @param {Humanloop.UpdateEvaluationRunRequest} request
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.updateEvaluationRun("id", "run_id")
     */
    updateEvaluationRun(id: string, runId: string, request?: Humanloop.UpdateEvaluationRunRequest, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationRunResponse>;
    /**
     * Add the specified Logs to a Run.
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {string} runId - Unique identifier for Run.
     * @param {Humanloop.AddLogsToRunRequest} request
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.addLogsToRun("id", "run_id", {
     *         logIds: ["log_ids"]
     *     })
     */
    addLogsToRun(id: string, runId: string, request: Humanloop.AddLogsToRunRequest, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationRunResponse>;
    /**
     * Get Evaluation Stats.
     *
     * Retrieve aggregate stats for the specified Evaluation. This includes the number of generated Logs for each Run and the
     * corresponding Evaluator statistics (such as the mean and percentiles).
     *
     * @param {string} id - Unique identifier for Evaluation.
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.getStats("id")
     */
    getStats(id: string, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.EvaluationStats>;
    /**
     * Get the Logs associated to a specific Evaluation.
     *
     * This returns the Logs associated to all Runs within with the Evaluation.
     *
     * @param {string} id - String ID of evaluation. Starts with `ev_` or `evr_`.
     * @param {Humanloop.GetLogsEvaluationsIdLogsGetRequest} request
     * @param {Evaluations.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Humanloop.UnprocessableEntityError}
     *
     * @example
     *     await client.evaluations.getLogs("id")
     */
    getLogs(id: string, request?: Humanloop.GetLogsEvaluationsIdLogsGetRequest, requestOptions?: Evaluations.RequestOptions): Promise<Humanloop.PaginatedDataEvaluationLogResponse>;
    protected _getCustomAuthorizationHeaders(): Promise<{
        "X-API-KEY": string | undefined;
    }>;
}
