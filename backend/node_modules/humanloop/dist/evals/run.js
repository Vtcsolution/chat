"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.runEval = runEval;
/**
 * Evaluation utils for the Humanloop SDK.
 *
 * This module provides a set of utilities to aid running Eval workflows on Humanloop
 * where you are managing the runtime of your application in your code.
 *
 * Functions in this module should be accessed via the Humanloop client. They should
 * not be called directly.
 */
const cli_progress_1 = __importDefault(require("cli-progress"));
const lodash_1 = require("lodash");
const context_1 = require("../context");
const error_1 = require("../error");
// ANSI escape codes for logging colors
const YELLOW = "\x1b[93m";
const CYAN = "\x1b[96m";
const GREEN = "\x1b[92m";
const RED = "\x1b[91m";
const RESET = "\x1b[0m";
/**
 * Maps over an array of items with a concurrency limit, applying an asynchronous mapper function to each item.
 *
 * @template T - The type of the items in the input array.
 * @template O - The type of the items in the output array.
 *
 * @param {T[]} iterable - The array of items to be mapped.
 * @param {(item: T) => Promise<O>} mapper - The asynchronous function to apply to each item.
 * @param {{ concurrency: number }} options - Options for the mapping operation.
 * @param {number} options.concurrency - The maximum number of promises to resolve concurrently.
 *
 * @returns {Promise<O[]>} A promise that resolves to an array of mapped items.
 *
 * @throws {TypeError} If the first argument is not an array.
 * @throws {TypeError} If the second argument is not a function.
 * @throws {TypeError} If the concurrency option is not a positive number.
 *
 * @description
 * The `pMap` function processes the input array in chunks, where the size of each chunk is determined by the `concurrency` option.
 * This controls how many promises are resolved at a time, which can help avoid issues such as rate limit errors when making server requests.
 */
function pMap(iterable, mapper, options) {
    return __awaiter(this, void 0, void 0, function* () {
        const { concurrency } = options;
        if (!Array.isArray(iterable)) {
            throw new TypeError("Expected the first argument to be an array");
        }
        if (typeof mapper !== "function") {
            throw new TypeError("Expected the second argument to be a function");
        }
        if (typeof concurrency !== "number" || concurrency <= 0) {
            throw new TypeError("Expected the concurrency option to be a positive number");
        }
        const result = [];
        for (let i = 0; i < iterable.length; i += concurrency) {
            const chunk = iterable.slice(i, i + concurrency);
            try {
                const chunkResults = yield Promise.all(chunk.map(mapper));
                result.push(...chunkResults);
            }
            catch (error) {
                // Handle individual chunk errors if necessary
                // For now, rethrow to reject the entire pMap promise
                throw error;
            }
        }
        return result;
    });
}
function callableIsHumanloopUtility(file) {
    return file.callable !== undefined && "decorator" in file.callable;
}
function fileOrFileInsideHLUtility(file) {
    if (callableIsHumanloopUtility(file)) {
        // @ts-ignore
        const innerFile = file.callable.file;
        if ("path" in file && innerFile.path !== file.path) {
            throw new error_1.HumanloopRuntimeError("`path` attribute specified in the `file` does not match the path of the decorated function. " +
                `Expected \`${innerFile.path}\`, got \`${file.path}\`.`);
        }
        if ("id" in file) {
            throw new error_1.HumanloopRuntimeError("Do not specify an `id` attribute in `file` argument when using a decorated function.");
        }
        if ("version" in file) {
            if (innerFile.type !== "prompt") {
                throw new error_1.HumanloopRuntimeError(`Do not specify a \`version\` attribute in \`file\` argument when using a ${(0, lodash_1.capitalize)(innerFile.type)} decorated function.`);
            }
        }
        if ("type" in file && innerFile.type !== file.type) {
            throw new error_1.HumanloopRuntimeError("Attribute `type` of `file` argument does not match the file type of the decorated function. " +
                `Expected \`${innerFile.type}\`, got \`${file.type}\`.`);
        }
        const file_ = Object.assign({}, innerFile);
        if (file_.type === "prompt") {
            console.warn(`${YELLOW}` +
                "The @prompt decorator will not spy on provider calls when passed to `evaluations.run()`. " +
                "Using the `version` in `file` argument instead.\n" +
                `${RESET}`);
            file_.version = file.version;
        }
        return file_;
    }
    else {
        const file_ = Object.assign({}, file);
        if (!file_.path && !file_.id) {
            throw new error_1.HumanloopRuntimeError("You must provide a path or id in your `file`.");
        }
        return file_;
    }
}
function getFileType(file) {
    // Determine the `type` of the `file` to Evaluate - if not `type` provided, default to `flow`
    try {
        const type_ = file.type;
        console.info(`${CYAN}Evaluating your ${type_} function corresponding to \`${file.path || file.id}\` on Humanloop${RESET}\n\n`);
        return type_ || "flow";
    }
    catch (error) {
        const type_ = "flow";
        console.warn(`${YELLOW}No \`file\` type specified, defaulting to flow.${RESET}\n`);
        return type_;
    }
}
function getFileCallable(file, type_) {
    // Get the `callable` from the `file` to Evaluate
    const function_ = file.callable;
    if (!function_) {
        if (type_ === "flow") {
            throw new Error("You must provide a `callable` for your Flow `file` to run a local eval.");
        }
        else {
            console.info(`${CYAN}No \`callable\` provided for your ${type_} file - will attempt to generate logs on Humanloop.\n\n${RESET}`);
        }
    }
    return function_;
}
function runEval(client_1, file_1, dataset_1, name_1) {
    return __awaiter(this, arguments, void 0, function* (client, file, dataset, name, evaluators = [], concurrency = 8) {
        if (concurrency > 32) {
            console.log("Warning: Too many parallel workers, capping the number to 32.");
        }
        concurrency = Math.min(concurrency, 32);
        const file_ = fileOrFileInsideHLUtility(file);
        const type_ = getFileType(file_);
        const function_ = getFileCallable(file_, type_);
        if (function_ && "file" in function_) {
            // @ts-ignore
            const decoratorType = function_.file.type;
            if (decoratorType !== type_) {
                throw new error_1.HumanloopRuntimeError(`The type of the decorated function does not match the type of the file. Expected \`${(0, lodash_1.capitalize)(type_)}\`, got \`${(0, lodash_1.capitalize)(decoratorType)}\`.`);
            }
        }
        let hlFile;
        try {
            hlFile = yield upsertFile({ file: file_, type: type_, client: client });
        }
        catch (e) {
            console.error(`${RED}Error in your \`file\` argument:\n\n${e.constructor.name}: ${e.message}${RESET}`);
            return [];
        }
        let hlDataset;
        try {
            hlDataset = yield upsertDataset({ dataset: dataset, client: client });
        }
        catch (e) {
            console.error(`${RED}Error in your \`file\` argument:\n\n${e.constructor.name}: ${e.message}${RESET}`);
            return [];
        }
        let localEvaluators;
        try {
            localEvaluators = yield upsertLocalEvaluators({
                evaluators: evaluators.filter((evaluator) => "callable" in evaluator),
                client: client,
                // @ts-ignore
                callable: function_,
                type: type_,
            });
        }
        catch (e) {
            console.error(`${RED}Error in your \`file\` argument:\n\n${e.constructor.name} ${e.message}${RESET}`);
            return [];
        }
        assertDatasetEvaluatorsFit(hlDataset, localEvaluators);
        const { evaluation, run } = yield getNewRun({
            client,
            evaluationName: name,
            evaluators,
            hlFile,
            hlDataset,
            // @ts-ignore
            func: function_,
        });
        const runId = run.id;
        function cancelEvaluation() {
            client.evaluations.updateEvaluationRun(evaluation.id, runId, {
                status: "cancelled",
            });
        }
        function handleExitSignal(signum) {
            process.stderr.write(`\n${RED}Received signal ${signum}, cancelling Evaluation and shutting down threads...${RESET}\n`);
            cancelEvaluation();
            process.exit(signum);
        }
        process.on("SIGINT", handleExitSignal);
        process.on("SIGTERM", handleExitSignal);
        // Header of the CLI report
        console.log(`\n${CYAN}Navigate to your Evaluation:${RESET}\n${evaluation.url}\n`);
        console.log(`${CYAN}${type_.charAt(0).toUpperCase() + type_.slice(1)} Version ID: ${hlFile.versionId}${RESET}`);
        console.log(`${CYAN}Run ID: ${runId}${RESET}`);
        // Generate locally if a file `callable` is provided
        if (function_ === undefined) {
            // TODO: trigger run when updated API is available
            process.stdout.write(`${CYAN}\nRunning '${hlFile.name}' over the Dataset '${hlDataset.name}'${RESET}\n`);
        }
        else {
            // Running the evaluation locally
            process.stdout.write(`${CYAN}\nRunning '${hlFile.name}' over the Dataset '${hlDataset.name}' locally...${RESET}\n\n`);
        }
        // Configure the progress bar
        const progressBar = new cli_progress_1.default.SingleBar({
            format: "Progress |" +
                "{bar}" +
                "| {percentage}% || {value}/{total} Datapoints",
        }, cli_progress_1.default.Presets.shades_classic);
        function processDatapoint(datapoint, runId) {
            return __awaiter(this, void 0, void 0, function* () {
                function uploadCallback(logId) {
                    return __awaiter(this, void 0, void 0, function* () {
                        yield runLocalEvaluators(client, logId, datapoint, localEvaluators);
                        progressBar.increment();
                    });
                }
                if (function_ === undefined) {
                    throw new error_1.HumanloopRuntimeError(`\`file.callable\` is undefined. Please provide a callable for your file.`);
                }
                const logFunc = getLogFunction(client, type_, hlFile.id, hlFile.versionId, runId);
                if (datapoint.inputs === undefined) {
                    throw new Error(`Datapoint 'inputs' attribute is undefined.`);
                }
                context_1.HL_CONTEXT.with((0, context_1.setEvaluationContext)(new context_1.EvaluationContext({
                    sourceDatapointId: datapoint.id,
                    runId,
                    evalCallback: uploadCallback,
                    fileId: hlFile.id,
                    path: hlFile.path,
                })), () => __awaiter(this, void 0, void 0, function* () {
                    const startTime = new Date();
                    let funcInputs = Object.assign({}, datapoint.inputs);
                    if ("messages" in datapoint) {
                        funcInputs = Object.assign(Object.assign({}, funcInputs), { messages: datapoint.messages });
                    }
                    const evaluationContext = (0, context_1.getEvaluationContext)();
                    if (!evaluationContext) {
                        throw new Error("Internal error: evaluation context is not set while processing a datapoint.");
                    }
                    try {
                        const output = yield callFunction(function_, datapoint);
                        if (!evaluationContext.logged) {
                            const log = yield logFunc({
                                inputs: Object.assign({}, datapoint.inputs),
                                messages: datapoint.messages,
                                output: output,
                                sourceDatapointId: datapoint.id,
                                runId: runId,
                                startTime: startTime,
                                endTime: new Date(),
                                logStatus: "complete",
                            });
                            // @ts-ignore
                            evaluationContext._callback(log.id);
                        }
                    }
                    catch (e) {
                        if (e instanceof error_1.HumanloopRuntimeError) {
                            throw e;
                        }
                        const errorMessage = e instanceof Error ? e.message : String(e);
                        const log = yield logFunc({
                            inputs: Object.assign({}, datapoint.inputs),
                            messages: datapoint.messages,
                            error: errorMessage,
                            sourceDatapointId: datapoint.id,
                            runId: runId,
                            startTime: startTime,
                            endTime: new Date(),
                            logStatus: "complete",
                        });
                        // @ts-ignore
                        evaluationContext._callback(log.id);
                        console.warn(`\nYour ${type_}'s callable failed for Datapoint: ${datapoint.id}.\nError: ${errorMessage}`);
                    }
                }));
            });
        }
        // Generate locally if a function is provided
        if (function_) {
            console.log(`${CYAN}\nRunning ${hlFile.name} over the Dataset ${hlDataset.name}${RESET}`);
            const totalDatapoints = hlDataset.datapoints.length;
            progressBar.start(totalDatapoints, 0);
            yield pMap(hlDataset.datapoints, (datapoint) => __awaiter(this, void 0, void 0, function* () {
                yield processDatapoint(datapoint, runId);
            }), { concurrency: concurrency });
            progressBar.stop();
        }
        else {
            // TODO: trigger run when updated API is available
            console.log(`${CYAN}\nRunning ${hlFile.name} over the Dataset ${hlDataset.name}${RESET}`);
        }
        // Wait for the Evaluation to complete then print the results
        let stats;
        do {
            stats = yield client.evaluations.getStats(evaluation.id);
            if (stats === null || stats === void 0 ? void 0 : stats.progress) {
                const newLines = stats.progress.split("\n").length - 1;
                process.stdout.write(`\r${stats.progress}`);
                // Move the cursor up by the number of new lines
                process.stdout.moveCursor(0, -newLines);
            }
            if (stats.status !== "completed") {
                yield new Promise((resolve) => setTimeout(resolve, 500));
            }
        } while (stats.status !== "completed");
        console.log(stats.report);
        const checks = [];
        if (evaluators.some((evaluator) => evaluator.threshold !== undefined) ||
            stats.runStats.length > 1) {
            for (const evaluator of evaluators) {
                const [_, score, delta] = checkEvaluationImprovement(evaluation, evaluator.path, stats, runId);
                let thresholdCheck = undefined;
                if (evaluator.threshold !== undefined) {
                    thresholdCheck = score >= evaluator.threshold;
                    thresholdCheck = checkEvaluationThreshold(evaluation, stats, evaluator.path, evaluator.threshold, runId);
                }
                checks.push({
                    path: evaluator.path,
                    // TODO: Add back in with number valence on Evaluators
                    // improvementCheck: improvementCheck,
                    score: score,
                    delta: delta,
                    threshold: evaluator.threshold,
                    thresholdCheck: thresholdCheck,
                    evaluationId: evaluation.id,
                });
            }
        }
        console.info(`\n${CYAN}View your Evaluation:${RESET}\n${evaluation.url}\n`);
        return checks;
    });
}
function callFunction(callable, datapoint) {
    return __awaiter(this, void 0, void 0, function* () {
        const datapointDict = Object.assign({}, datapoint);
        let output;
        if (callable === undefined) {
            throw new error_1.HumanloopRuntimeError(`\`file.callable\` is undefined. Please provide a callable for your file.`);
        }
        if ("messages" in datapointDict && !!datapointDict["messages"]) {
            output = yield callable(Object.assign(Object.assign({}, datapointDict["inputs"]), { messages: datapointDict["messages"] }));
        }
        else {
            output = yield callable(Object.assign({}, datapointDict["inputs"]));
        }
        if (typeof output !== "string") {
            try {
                output = JSON.stringify(output);
            }
            catch (error) {
                throw new error_1.HumanloopRuntimeError(`\`file.callable\` must return a string or a JSON serializable object.`);
            }
        }
        return output;
    });
}
function upsertFile(_a) {
    return __awaiter(this, arguments, void 0, function* ({ file, type, client, }) {
        // Get or create the file on Humanloop
        const version = file.version || {};
        const fileDict = Object.assign(Object.assign({}, file), version);
        let hlFile;
        switch (type) {
            case "flow":
                // Be more lenient with Flow versions as they are arbitrary json
                const flowVersion = { attributes: version };
                const fileDictWithFlowVersion = Object.assign(Object.assign({}, file), flowVersion);
                hlFile = yield client.flows.upsert(fileDictWithFlowVersion);
                break;
            case "prompt":
                hlFile = yield client.prompts.upsert(fileDict);
                break;
            case "tool":
                hlFile = yield client.tools.upsert(fileDict);
                break;
            case "evaluator":
                hlFile = yield client.evaluators.upsert(fileDict);
                break;
            default:
                throw new Error(`Unsupported File type: ${type}`);
        }
        return hlFile;
    });
}
function upsertDataset(_a) {
    return __awaiter(this, arguments, void 0, function* ({ dataset, client, }) {
        // Upsert the Dataset
        if (!dataset.action) {
            dataset.action = "set";
        }
        if (!dataset.datapoints) {
            dataset.datapoints = [];
            // Use `upsert` to get existing dataset ID if no datapoints provided, given we can't `get` on path.
            dataset.action = "add";
        }
        const hlDataset = yield client.datasets.upsert(Object.assign(Object.assign({}, dataset), { datapoints: dataset.datapoints || [] }));
        return yield client.datasets.get(hlDataset.id, {
            versionId: hlDataset.versionId,
            includeDatapoints: true,
        });
    });
}
function getNewRun(_a) {
    return __awaiter(this, arguments, void 0, function* ({ client, evaluationName, evaluators, hlFile, hlDataset, func, }) {
        var _b, e_1, _c, _d;
        // Get or create the Evaluation based on the name
        let evaluation = null;
        try {
            evaluation = yield client.evaluations.create({
                name: evaluationName,
                evaluators: evaluators.map((evaluator) => ({
                    path: evaluator.path,
                    id: evaluator.id,
                })),
                file: { id: hlFile.id },
            });
        }
        catch (error) {
            // If the name exists, go and get it
            // TODO: Update API GET to allow querying by name and file.
            if (error.statusCode === 409) {
                const evals = yield client.evaluations.list({
                    fileId: hlFile.id,
                    size: 50,
                });
                try {
                    for (var _e = true, evals_1 = __asyncValues(evals), evals_1_1; evals_1_1 = yield evals_1.next(), _b = evals_1_1.done, !_b; _e = true) {
                        _d = evals_1_1.value;
                        _e = false;
                        const e = _d;
                        if (e.name === evaluationName) {
                            evaluation = e;
                            break;
                        }
                    }
                }
                catch (e_1_1) { e_1 = { error: e_1_1 }; }
                finally {
                    try {
                        if (!_e && !_b && (_c = evals_1.return)) yield _c.call(evals_1);
                    }
                    finally { if (e_1) throw e_1.error; }
                }
            }
            else {
                throw error;
            }
            if (!evaluation) {
                throw new Error(`Evaluation with name ${evaluationName} not found.`);
            }
        }
        // Create a new Run
        const run = yield client.evaluations.createRun(evaluation.id, {
            dataset: { versionId: hlDataset.versionId },
            version: { versionId: hlFile.versionId },
            orchestrated: func ? false : true,
            useExistingLogs: false,
        });
        return { evaluation, run };
    });
}
function upsertLocalEvaluators(_a) {
    return __awaiter(this, arguments, void 0, function* ({ evaluators, callable, type, client, }) {
        // Upsert the local Evaluators; other Evaluators are just referenced by `path` or `id`
        const localEvaluators = [];
        if (evaluators) {
            for (const evaluatorRequest of evaluators) {
                // If a callable is provided for an Evaluator, we treat it as External
                const evalFunction = evaluatorRequest.callable;
                if (evalFunction !== undefined) {
                    // TODO: support the case where `file` logs generated on Humanloop but Evaluator logs generated locally
                    if (callable === undefined) {
                        throw new Error(`Local Evaluators are only supported when generating Logs locally using your ${type}'s callable. Please provide a callable for your file in order to run Evaluators locally.`);
                    }
                    const spec = {
                        argumentsType: evaluatorRequest.argsType,
                        returnType: evaluatorRequest.returnType,
                        attributes: { code: evalFunction.toString() },
                        evaluatorType: "external",
                    };
                    const evaluator = yield client.evaluators.upsert({
                        id: evaluatorRequest.id,
                        path: evaluatorRequest.path,
                        spec,
                    });
                    localEvaluators.push({
                        hlEvaluator: evaluator,
                        // @ts-ignore
                        function: evalFunction,
                    });
                }
            }
        }
        return localEvaluators;
    });
}
function assertDatasetEvaluatorsFit(hlDataset, localEvaluators) {
    var _a;
    // Validate upfront that the local Evaluators and Dataset fit
    const requiresTarget = localEvaluators.some((localEvaluator) => localEvaluator.hlEvaluator.spec.argumentsType === "target_required");
    if (requiresTarget) {
        const missingTargets = (hlDataset.datapoints || []).filter((datapoint) => !datapoint.target).length;
        if (missingTargets > 0) {
            throw new Error(`${missingTargets} Datapoints have no target. A target is required for the Evaluator: ${(_a = localEvaluators.find((localEvaluator) => localEvaluator.hlEvaluator.spec.argumentsType ===
                "target_required")) === null || _a === void 0 ? void 0 : _a.hlEvaluator.path}`);
        }
    }
}
/**
 * Returns the appropriate log function pre-filled with common parameters.
 *
 * @param client - The HumanloopClient instance used to make API calls.
 * @param type - The type of file for which the log function is being generated. Can be "flow", "prompt", or "tool".
 * @param fileId - The ID of the file.
 * @param versionId - The version ID of the file.
 * @param runId - The run ID associated with the log.
 * @returns A function that logs data to the appropriate endpoint based on the file type.
 * @throws {Error} If the provided file type is unsupported.
 */
function getLogFunction(client, type, fileId, versionId, runId) {
    const logRequest = {
        // TODO: why does the Log `id` field refer to the file ID in the API?
        // Why are both `id` and `version_id` needed in the API?
        id: fileId,
        versionId,
        runId,
    };
    switch (type) {
        case "flow":
            return (args) => __awaiter(this, void 0, void 0, function* () {
                // @ts-ignore Using the original method instead of the overloaded one
                return yield client.flows._log(Object.assign(Object.assign(Object.assign({}, logRequest), { logStatus: "complete" }), args));
            });
        case "prompt":
            return (args) => __awaiter(this, void 0, void 0, function* () { 
            // @ts-ignore Using the original method instead of the overloaded one
            return yield client.prompts._log(Object.assign(Object.assign({}, logRequest), args)); });
        // case "evaluator":
        //     return (args: CreateEvaluatorLogRequest) =>
        //         client.evaluators._log({ ...logRequest, ...args });
        case "tool":
            return (args) => __awaiter(this, void 0, void 0, function* () { 
            // @ts-ignore Using the original method instead of the overloaded one
            return yield client.tools._log(Object.assign(Object.assign({}, logRequest), args)); });
        default:
            throw new Error(`Unsupported File version: ${type}`);
    }
}
function runLocalEvaluators(client, logId, datapoint, localEvaluators) {
    return __awaiter(this, void 0, void 0, function* () {
        const log = yield client.logs.get(logId);
        const promises = localEvaluators.map((_a) => __awaiter(this, [_a], void 0, function* ({ hlEvaluator, function: evalFunction }) {
            const startTime = new Date();
            let judgment;
            try {
                if (hlEvaluator.spec.argumentsType === "target_required") {
                    judgment = yield evalFunction({ log, datapoint });
                }
                else {
                    judgment = yield evalFunction({ log });
                }
                // @ts-ignore Using the original method instead of the overloaded one
                yield client.evaluators._log({
                    path: hlEvaluator.path,
                    versionId: hlEvaluator.versionId,
                    parentId: logId,
                    judgment: judgment,
                    startTime: startTime,
                    endTime: new Date(),
                });
            }
            catch (e) {
                // @ts-ignore Using the original method instead of the overloaded one
                yield client.evaluators._log({
                    path: hlEvaluator.path,
                    versionId: hlEvaluator.versionId,
                    parentId: logId,
                    error: e instanceof Error ? e.message : String(e),
                    startTime: startTime,
                    endTime: new Date(),
                });
                console.warn(`\nEvaluator ${hlEvaluator.path} failed with error ${e}`);
            }
        }));
        yield Promise.all(promises);
    });
}
function checkEvaluationImprovement(evaluation, evaluatorPath, stats, runId) {
    const runStats = stats.runStats.find((run) => run.runId === runId);
    if (!runStats) {
        throw new Error(`Run ${runId} not found in Evaluation ${evaluation.id}`);
    }
    const latestEvaluatorStatsByPath = getEvaluatorStatsByPath(runStats, evaluation);
    if (stats.runStats.length == 1) {
        console.log(`${YELLOW}⚠️ No previous versions to compare with.${RESET}`);
        return [true, 0, 0];
    }
    // Latest Run is at index 0, previous Run is at index 1
    const previousEvaluatorStatsByPath = getEvaluatorStatsByPath(stats.runStats[1], evaluation);
    if (evaluatorPath in latestEvaluatorStatsByPath &&
        evaluatorPath in previousEvaluatorStatsByPath) {
        const latestEvaluatorStats = latestEvaluatorStatsByPath[evaluatorPath];
        const previousEvaluatorStats = previousEvaluatorStatsByPath[evaluatorPath];
        const latestScore = getScoreFromEvaluatorStat(latestEvaluatorStats);
        const previousScore = getScoreFromEvaluatorStat(previousEvaluatorStats);
        if (latestScore === null || previousScore === null) {
            throw new Error(`Could not find score for Evaluator ${evaluatorPath}`);
        }
        let diff = latestScore - previousScore;
        // Round to 2 decimal places
        diff = Math.round(diff * 100) / 100;
        console.log(`${CYAN}Change of [${diff}] for Evaluator ${evaluatorPath}${RESET}`);
        return [diff >= 0, latestScore, diff];
    }
    else {
        throw Error(`Evaluator ${evaluatorPath} not found in the stats.`);
    }
}
function getScoreFromEvaluatorStat(stat) {
    let score = null;
    if ("numTrue" in stat) {
        score =
            stat.numTrue /
                stat.totalLogs;
        // Round to 2 decimal places
        score = Math.round(score * 100) / 100;
    }
    else if ("mean" in stat && stat.mean !== undefined) {
        // Round to 2 decimal places
        score = Math.round(stat.mean * 100) / 100;
    }
    else {
        throw new Error(`Unexpected EvaluatorStats type: ${stat}`);
    }
    return score;
}
function getEvaluatorStatsByPath(stats, evaluation) {
    const evaluatorsById = {};
    for (const evaluator of evaluation.evaluators) {
        evaluatorsById[evaluator.version.versionId] = evaluator;
    }
    const evaluatorStatsByPath = {};
    for (const evaluatorStats of stats.evaluatorStats) {
        const evaluator = evaluatorsById[evaluatorStats.evaluatorVersionId];
        evaluatorStatsByPath[evaluator.version.path] = evaluatorStats;
    }
    return evaluatorStatsByPath;
}
function checkEvaluationThreshold(evaluation, stats, evaluatorPath, threshold, runId) {
    const runStats = stats.runStats.find((run) => run.runId === runId);
    if (!runStats) {
        throw new Error(`Run ${runId} not found in Evaluation ${evaluation.id}`);
    }
    const evaluatorStatsByPath = getEvaluatorStatsByPath(runStats, evaluation);
    if (evaluatorPath in evaluatorStatsByPath) {
        const evaluatorStats = evaluatorStatsByPath[evaluatorPath];
        const score = getScoreFromEvaluatorStat(evaluatorStats);
        if (score === null) {
            throw new Error(`Could not find score for Evaluator ${evaluatorPath}`);
        }
        if (score >= threshold) {
            console.log(`${GREEN}✅ Latest eval [${score}] above threshold [${threshold}] for Evaluator ${evaluatorPath}.${RESET}`);
        }
        else {
            console.log(`${RED}❌ Latest eval [${score}] below threshold [${threshold}] for Evaluator ${evaluatorPath}.${RESET}`);
        }
        return score >= threshold;
    }
    else {
        throw new Error(`Evaluator ${evaluatorPath} not found in the stats.`);
    }
}
