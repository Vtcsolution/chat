"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.HumanloopClient = void 0;
const resources_1 = require("@opentelemetry/resources");
const sdk_trace_node_1 = require("@opentelemetry/sdk-trace-node");
const instrumentation_anthropic_1 = require("@traceloop/instrumentation-anthropic");
const instrumentation_cohere_1 = require("@traceloop/instrumentation-cohere");
const instrumentation_openai_1 = require("@traceloop/instrumentation-openai");
const Client_1 = require("./Client");
const Client_2 = require("./api/resources/evaluations/client/Client");
const flow_1 = require("./decorators/flow");
const prompt_1 = require("./decorators/prompt");
const tool_1 = require("./decorators/tool");
const environments_1 = require("./environments");
const error_1 = require("./error");
const run_1 = require("./evals/run");
const exporter_1 = require("./otel/exporter");
const processor_1 = require("./otel/processor");
const overload_1 = require("./overload");
const version_1 = require("./version");
const RED = "\x1b[91m";
const RESET = "\x1b[0m";
class ExtendedEvaluations extends Client_2.Evaluations {
    constructor(options, client) {
        super(options);
        this._client = client;
    }
    /**
     * Evaluate a File's performance.
     *
     * The utility takes a callable function that will be run over the dataset. The function's inputs and outputs are transformed into a Log of the evaluated File. The Log is the passed to the Evaluators to produce metrics.
     *
     * Running the file again with the same Dataset and Evaluators will create a new Run in the Evaluation. The new Run will be compared to the previous Runs, allowing you to iterate on your File.
     *
     * ```typescript
     * async function trueOrFalse(query: string): Promise<boolean> {
     *   const response = await openAIClient.chat.completions.create({
     *     model: "gpt-4o-mini",
     *     temperature: 0,
     *     messages: [
     *       { role: "system", content: "You are a helpful assistant. You must evaluate queries and decide if their sentiment is closer to boolean true or boolean false. Output only 'true' or 'false' and nothing else" },
     *       { role: "user", content: query }
     *     ]
     *   });
     *   return response.choices[0].message.content === 'true';
     * }
     *
     * humanloop.evaluations.run({
     *   type: "flow",
     *   callable: trueOrFalse,
     *   path: "Project/True or False"
     * },
     * {
     *   path: "Project/Fuzzy Logic 101",
     *   datapoints: [
     *     { inputs: { query: "This is 100%" }, target: { output: true } },
     *     { inputs: { query: "I don't think so" }, target: { output: false } },
     *     { inputs: { query: "That doesn't go around here" }, target: { output: false } },
     *     { inputs: { query: "Great work bot!" }, target: { output: true } },
     *     { inputs: { query: "I agree" }, target: { output: true } }
     *   ]
     * },
     * "Accuracy Evaluation",
     * evaluators: [
     *   {
     *     callable: (log, datapoint) => log.output === datapoint.target.output,
     *     path: "Project/Accuracy Evaluator"
     *   }
     * ]
     * );
     * ```
     *
     * @param file - The file to evaluate.
     * @param file.type - The type of file being evaluated e.g. "flow".
     * @param file.version - The version of the file being evaluated.
     * @param file.callable - The callable to run over the dataset. Can also be a File-utility wrapped callable.
     * @param dataset - The dataset used in evaluation. Can be an online dataset or local data can be provided as an array of datapoints.
     * @param dataset.path - The path of the dataset to use in evaluation. If the Dataset is stored on Humanloop, you only need to provide the path.
     * @param dataset.datapoints - The datapoints to map your function over to produce the outputs required by the evaluation. The datapoints will be uploaded to Humanloop and create a new version of the Dataset.
     * @param name - The name of the evaluation.
     * @param evaluators - List of evaluators to be. Can be ran on Humanloop if specified only by path, or locally if a callable is provided.
     * @param concurrency - Number of datapoints to process in parallel.
     */
    run(_a) {
        return __awaiter(this, arguments, void 0, function* ({ file, dataset, name, evaluators = [], concurrency = 8, }) {
            return (0, run_1.runEval)(this._client, file, dataset, name, evaluators, concurrency);
        });
    }
}
class HumanloopTracerSingleton {
    constructor(config) {
        var _a, _b, _c;
        this.tracerProvider = new sdk_trace_node_1.NodeTracerProvider({
            resource: (0, resources_1.resourceFromAttributes)({
                "service.name": "humanloop-typescript-sdk",
                "service.version": version_1.SDK_VERSION,
            }),
            spanProcessors: [
                new processor_1.HumanloopSpanProcessor(new exporter_1.HumanloopSpanExporter({
                    hlClientHeaders: {
                        "X-API-KEY": config.hlClientApiKey,
                        "X-Fern-Language": "Typescript",
                        "X-Fern-SDK-Name": "humanloop",
                        "X-Fern-SDK-Version": version_1.SDK_VERSION,
                    },
                    hlClientBaseUrl: config.hlClientBaseUrl,
                })),
            ],
        });
        if ((_a = config.instrumentProviders) === null || _a === void 0 ? void 0 : _a.OpenAI) {
            const openaiInstrumentation = new instrumentation_openai_1.OpenAIInstrumentation({
                enrichTokens: true,
            });
            openaiInstrumentation.manuallyInstrument(config.instrumentProviders.OpenAI);
            openaiInstrumentation.setTracerProvider(this.tracerProvider);
            openaiInstrumentation.enable();
        }
        if ((_b = config.instrumentProviders) === null || _b === void 0 ? void 0 : _b.Anthropic) {
            const anthropicInstrumentation = new instrumentation_anthropic_1.AnthropicInstrumentation();
            anthropicInstrumentation.manuallyInstrument(config.instrumentProviders.Anthropic);
            anthropicInstrumentation.setTracerProvider(this.tracerProvider);
            anthropicInstrumentation.enable();
        }
        if ((_c = config.instrumentProviders) === null || _c === void 0 ? void 0 : _c.CohereAI) {
            const cohereInstrumentation = new instrumentation_cohere_1.CohereInstrumentation();
            cohereInstrumentation.manuallyInstrument(config.instrumentProviders.CohereAI);
            cohereInstrumentation.setTracerProvider(this.tracerProvider);
            cohereInstrumentation.enable();
        }
        this.tracer = this.tracerProvider.getTracer("humanloop.sdk");
    }
    static getInstance(config) {
        if (!HumanloopTracerSingleton.instance) {
            HumanloopTracerSingleton.instance = new HumanloopTracerSingleton(config);
        }
        return HumanloopTracerSingleton.instance;
    }
}
class HumanloopClient extends Client_1.HumanloopClient {
    get opentelemetryTracer() {
        var _a;
        return HumanloopTracerSingleton.getInstance({
            hlClientApiKey: this.options().apiKey.toString(),
            hlClientBaseUrl: ((_a = this.options().baseUrl) === null || _a === void 0 ? void 0 : _a.toString()) ||
                environments_1.HumanloopEnvironment.Default.toString(),
            instrumentProviders: this.instrumentProviders,
        }).tracer;
    }
    /**
     * Constructs a new instance of the Humanloop client.
     *
     * @param _options - The base options for the Humanloop client.
     * @param _options.instrumentProviders - LLM provider modules to instrument. Allows the prompt decorator to spy on provider calls and log them to Humanloop
     *
     * Pass LLM provider modules as such:
     *
     * ```typescript
     * import { OpenAI } from "openai";
     * import { Anthropic } from "anthropic";
     * import { HumanloopClient } from "humanloop";
     *
     * const humanloop = new HumanloopClient({
     *     apiKey: process.env.HUMANLOOP_KEY,
     *     instrumentProviders: { OpenAI, Anthropic },
     * });
     *
     * const openai = new OpenAI({apiKey: process.env.OPENAI_KEY});
     * const anthropic = new Anthropic({apiKey: process.env.ANTHROPIC_KEY});
     * ```
     */
    constructor(_options) {
        var _a;
        super(_options);
        this.instrumentProviders = _options.instrumentProviders || {};
        this._prompts_overloaded = (0, overload_1.overloadLog)(super.prompts);
        this._prompts_overloaded = (0, overload_1.overloadCall)(this._prompts_overloaded);
        this._tools_overloaded = (0, overload_1.overloadLog)(super.tools);
        this._flows_overloaded = (0, overload_1.overloadLog)(super.flows);
        this._evaluators_overloaded = (0, overload_1.overloadLog)(super.evaluators);
        this._evaluations = new ExtendedEvaluations(_options, this);
        // Initialize the tracer singleton
        HumanloopTracerSingleton.getInstance({
            hlClientApiKey: this.options().apiKey.toString(),
            hlClientBaseUrl: ((_a = this.options().baseUrl) === null || _a === void 0 ? void 0 : _a.toString()) ||
                environments_1.HumanloopEnvironment.Default.toString(),
            instrumentProviders: this.instrumentProviders,
        });
    }
    options() {
        return this._options;
    }
    // Check if user has passed the LLM provider instrumentors
    assertAtLeastOneProviderModuleSet() {
        const userDidNotPassProviders = Object.values(this.instrumentProviders).every((provider) => !provider);
        if (userDidNotPassProviders) {
            throw new error_1.HumanloopRuntimeError(`${RED}To use the @prompt decorator, pass your LLM client library into the Humanloop client constructor. For example:\n\n
import { OpenAI } from "openai";
import { HumanloopClient } from "humanloop";

const humanloop = new HumanloopClient({apiKey: process.env.HUMANLOOP_KEY}, { OpenAI });
const openai = new OpenAI();
${RESET}`);
        }
    }
    /**
     * Auto-instrument LLM provider calls and create [Prompt](https://humanloop.com/docs/explanation/prompts)
     * Logs on Humanloop from them.
     *
     * ```typescript
     * import { OpenAI } from "openai";
     * import { Anthropic } from "anthropic";
     * import { HumanloopClient } from "humanloop";
     *
     * const humanloop = new HumanloopClient({
     *     apiKey: process.env.HUMANLOOP_KEY,
     *     instrumentProviders: { OpenAI, Anthropic },
     * });
     * const openai = new OpenAI({apiKey: process.env.OPENAI_KEY});
     *
     * const callOpenaiWithHumanloop = humanloop.prompt({
     *    path: "Chat Bot",
     *    callable: (args: {
     *      messages: ChatMessage[]
     *    }) => {
     *      const response = await openai.chat.completions.create({
     *        model: "gpt-4o",
     *        temperature: 0.8,
     *        frequency_penalty: 0.5,
     *        max_tokens: 200,
     *        messages: args.messages,
     *      });
     *      return response.choices[0].message.content;
     *    },
     * });
     *
     * const answer = await callOpenaiWithHumanloop({
     *   messages: [{ role: "user", content: "What is the capital of the moon?" }],
     * });
     *
     * // Calling the function above creates a new Log on Humanloop
     * // against this Prompt version:
     * {
     *     provider: "openai",
     *     model: "gpt-4o",
     *     endpoint: "chat",
     *     max_tokens: 200,
     *     temperature: 0.8,
     *     frequency_penalty: 0.5,
     * }
     * ```
     *
     * If a different model, endpoint, or hyperparameter is used, a new
     * Prompt version is created. For example:
     *
     * ```typescript
     * humanloopClient.prompt({
     *   path: "My Prompt",
     *   callable: async (messages: ChatMessage[]) => {
     *     const openaiClient = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
     *     const openaiResponse = await openaiClient.chat.completions.create({
     *       model: "gpt-4o-mini",
     *       temperature: 0.5,
     *     });
     *     const openaiContent = openaiResponse.choices[0].message.content;
     *
     *     const anthropicClient = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
     *     const anthropicResponse = await anthropicClient.messages.create({
     *       model: "claude-3-5-sonnet-20240620",
     *       temperature: 0.5,
     *     });
     *     const anthropicContent = anthropicResponse.content;
     *
     *     return { openaiContent, anthropicContent };
     *   }
     * });
     *
     * Calling this function will create two versions of the same Prompt:
     * {
     *     provider: "openai",
     *     model: "gpt-4o-mini",
     *     endpoint: "chat",
     *     max_tokens: 200,
     *     temperature: 0.5,
     *     frequency_penalty: 0.5,
     * }
     *
     * {
     *     provider: "anthropic",
     *     model: "claude-3-5-sonnet-20240620",
     *     endpoint: "messages",
     *     temperature: 0.5,
     * }
     *
     * And one Log will be added to each version of the Prompt.
     * ```
     *
     * @param callable - The callable to wrap.
     * @param path - The path to the Prompt.
     */
    prompt(args) {
        this.assertAtLeastOneProviderModuleSet();
        // @ts-ignore
        return (0, prompt_1.promptDecoratorFactory)(args.path, args.callable);
    }
    /**
     * Auto-instrument LLM provider calls and create [Tool](https://humanloop.com/docs/explanation/tools)
     * Logs on Humanloop from them.
     *
     * You must provide a `version` argument specifying the JSON Schema of the Tool's inputs and outputs,
     * along with a callable that accepts the inputs and returns the outputs.
     *
     * ```typescript
     *
     * const calculator = humanloop_client.tool({
     *     callable: (inputs: { a: number; b: number }) => inputs.a + inputs.b,
     *     path: "Andrei QA/SDK TS/Calculator",
     *     version: {
     *         function: {
     *             name: "calculator",
     *             description: "Add two numbers",
     *             parameters: {
     *                 type: "object",
     *                 properties: {
     *                     a: { type: "number", required: true },
     *                     b: { type: "number", required: true },
     *                 },
     *             },
     *         },
     *     },
     * });
     * ```
     *
     * @param callable - The callable to wrap.
     * @param path - The path to the Tool.
     * @param version - The JSON Schema of the Tool's inputs and outputs, plus the optional Humanloop fields `attributes and `setupValues`. See API reference for details.
     */
    tool(args) {
        // @ts-ignore
        return (0, tool_1.toolUtilityFactory)(this.opentelemetryTracer, args.callable, args.version, args.path);
    }
    /**
     * Trace SDK logging calls through [Flows](https://humanloop.com/docs/explanation/flows).
     *
     * Use it as the entrypoint of your LLM feature. Logging calls like `prompts.call(...)`,
     * `tools.call(...)`, or other Humanloop decorators will be automatically added to the trace.
     *
     * Example:
     *
     * ```typescript
     * const callLLM = humanloop_client.prompt({
     *     path: "My Prompt",
     *     callable: (messages: ChatMessage[]) => {
     *         const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
     *         return client.chat.completions.create({
     *             model: "gpt-4o",
     *             temperature: 0.8,
     *             frequency_penalty: 0.5,
     *         max_tokens: 200,
     *         messages: messages,
     *     }).choices[0].message.content;
     * }
     *
     * const agent = humanloop_client.flow({
     *     callable: () => {
     *         while (true) {
     *             const messages: ChatMessage[] = [];
     *             const userInput = prompt("You: ");
     *             if (userInput === "exit") {
     *                 break;
     *             }
     *             messages.push({ role: "user", content: userInput });
     *             const response = callLLM(messages);
     *             messages.push({ role: "assistant", content: response });
     *             console.log(`Assistant: ${response}`);
     *         }
     *     },
     *     path: "My Flow",
     *     attributes: { version: "v1" },
     * });
     *
     * ```
     *
     * Each call to `agent` will create a trace corresponding to the conversation
     * session. Multiple Prompt Logs will be created as the LLM is called. They
     * will be added to the trace, allowing you to see the whole conversation
     * in the UI.
     *
     * If the function returns a ChatMessage-like object, the Log will
     * populate the `outputMessage` field. Otherwise, it will serialize
     * the return value and populate the `output` field.
     *
     * If an exception is raised, the output fields will be set to None
     * and the error message will be set in the Log's `error` field.
     *
     * @param path - The path to the Flow. If not provided, the function name
     *     will be used as the path and the File will be created in the root
     *     of your organization workspace.
     *
     * @param attributes - Additional fields to describe the Flow. Helpful to separate Flow versions from each other with details on how they were created or used.
     */
    flow({ callable, path, attributes, }) {
        // @ts-ignore
        return (0, flow_1.flowUtilityFactory)(this, this.opentelemetryTracer, callable, path, attributes);
    }
    get evaluations() {
        return this._evaluations;
    }
    get prompts() {
        return this._prompts_overloaded;
    }
    get flows() {
        return this._flows_overloaded;
    }
    get tools() {
        return this._tools_overloaded;
    }
    get evaluators() {
        return this._evaluators_overloaded;
    }
}
exports.HumanloopClient = HumanloopClient;
